[preset.0]

name="tlbtc_demo"
platform="Web"
runnable=true
advanced_options=false
dedicated_server=false
custom_features=""
export_filter="all_resources"
include_filter="env.txt"
exclude_filter=""
export_path="../../build/tlbtc_build/index.html"
encryption_include_filters=""
encryption_exclude_filters=""
encrypt_pck=false
encrypt_directory=false
script_export_mode=2

[preset.0.options]

custom_template/debug=""
custom_template/release=""
variant/extensions_support=true
variant/thread_support=false
vram_texture_compression/for_desktop=true
vram_texture_compression/for_mobile=false
html/export_icon=true
html/custom_html_shell=""
html/head_include="<script src=\"microsoft.cognitiveservices.speech.sdk.bundle.js\"></script>
<script>
	let visemeArr=[];
	let visemesReceived=false;
    let currentViseme = 1;
	let speechConfig;
	let synthesizer;
    const viseme_dict={
		0:\"BMP\",
		1:\"AEI\",
		2:\"AEI\",
		3:\"O\",
		4:\"AEI\", //or cd
		5:\"EE\",
		6:\"CHJSH\",
		7:\"WQ\",
		8:\"WQ\",
		9:\"O\",
		10:\"WQ\",
		11:\"AEI\",
		12:\"CHJSH\",
		13:\"R\",
		14:\"L\",
		15:\"CDNS\",
		16:\"CHJSH\",
		17:\"TH\",
		18:\"FV\",
		19:\"CDNS\",
		20:\"GK\",
		21:\"BMP\",
	};

     const frame_dict = {
        \"CHJSH\":3,
        \"TH\":10,
        \"WQ\":8,
        \"L\":6,
        \"CDNS\":2,
        \"U\":11,
        \"O\":7,
        \"FV\":5,
        \"BMP\":1,
        \"AEI\":0,
        \"R\":9,
        \"EE\":4,
		\"GK\":2
    }
		function initializeTTS(key) {
			// Azure Speech Service credentials
			const subscriptionKey = String(key);
			const region = \"eastus\"; 
	
			// Initialize Speech SDK configuration
			speechConfig = SpeechSDK.SpeechConfig.fromSubscription(subscriptionKey, region);
			speechConfig.speechSynthesisOutputFormat = SpeechSDK.SpeechSynthesisOutputFormat.Audio24Khz160KBitRateMonoWithMetadata;
			console.log('config_done');
		}
		
		
		
        // Function to handle speech synthesis
        function generateSpeech(text) {
			if (!text) return;
			visemeArr=[];
			//console.log('cleard old visemes',visemeArr);
			
            // SSML for speech synthesis
			const ssml = `<speak version='1.0' xml:lang='en-US' xmlns='http://www.w3.org/2001/10/synthesis' xmlns:mstts='http://www.w3.org/2001/mstts'> \\r\\n \\
				<voice name='en-US-JennyNeural'> \\r\\n \\
					<mstts:viseme type='redlips_front'/> \\r\\n \\
					${text} \\r\\n \\
					</voice> \\r\\n \\
					</speak>`;
					
			synthesizer = new SpeechSDK.SpeechSynthesizer(speechConfig);
			// Attach event listeners
			synthesizer.visemeReceived = (s, e) => {
				const visemeId = e.visemeId;
				const timestamp = e.audioOffset / 10000; // Convert nanoseconds to milliseconds
		visemeArr.push([visemeId,timestamp]);
		console.log(`Viseme: ${visemeMap[visemeId] || \"Unknown\"} (ID: ${visemeId}), Timestamp: ${timestamp}ms`);
	};
	
	synthesizer.synthesisStarted = () => console.log(\"Synthesis started...\");
	synthesizer.synthesisCompleted = () => {
		console.log(\"Synthesis completed.\");
		synthesizer.close(); // Clean up
            };
			
			synthesizer.speakSsmlAsync( 
						ssml,
						(result) => {
							if (result.reason === SpeechSDK.ResultReason.SynthesizingAudioCompleted) {
								console.log(\"Speech synthesis succeeded.\");
							} else {
								console.error(\"Speech synthesis failed:\", result.errorDetails);
							}
						},
						(error) => console.error(\"Error during synthesis:\", error)
					);
				}
    
        function testSample(){
			timeStamps.forEach( v => {
				var duration=v[1];
                setTimeout(() => {
					currentViseme=frame_dict[viseme_dict[v[0]]];
                    console.log(currentViseme);
                }, duration);
            })
        }

	async function playViseme() {
		console.log(\"Starting Viseme anims\");
		var index = 0;
		visemeArr.forEach( v => {
			var duration=v[1];
			setTimeout(() => {
				index++;
				window.ttsState='Speaking';
				currentViseme=frame_dict[viseme_dict[v[0]]];
				//console.log(currentViseme,viseme_dict[v[0]]);
				if (index==visemeArr.length) {
					window.ttsState=\"idle\";
					currentViseme = 1;
					if (state==1 || state==2) window.act_over = true;
					else {
						emotion = 'neutral';
					}
				}
				
			}, duration);
		});
		//console.log(visemeArr);
	}
	window.initializeTTS=initializeTTS;
	window.playViseme=playViseme;
	</script>
<script src=\"script.js\"></script>
<script>
	let aiResponse=\"\";
	let recognizedText = \"\"; 
	let ttsState = 'idle';
	
	const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
	recognition.lang = \"en-US\";
	recognition.continuous = true;
	recognition.interimResults = true;
	window.isListening=true;
	let isSpeaking=false;
	function startListening() {
		recognition.onstart = function() {
			console.log(\"Speech recognition started\");
			window.recognitionState = 'listening';
		};
		
		recognition.onend = function() {
		console.log(\"Speech recognition ended\");
		window.recognitionState = 'idle';
		// If Button is still held down
		if (window.isListening) recognition.start();
	};
	
	recognition.onresult = function(event) {
		recognizedText = \"\";               // Clear previous results for fresh display
		for (let i = 0; i < event.results.length; i++) {
			recognizedText += event.results[i][0].transcript;
		}
		console.log(\"Recognized Text (real-time):\", recognizedText);
		window.recognizedText = recognizedText;
		
	};
	
	recognition.onerror = function(event) {
		console.error(\"Speech recognition error:\", event.error);
		window.recognitionState = 'error';
	};

	recognition.start();
}

 function stopListening(){
	recognition.stop();
	window.isListening=false;
 }
 

// Request microphone access and set up the recorder
async function setupMicrophone() {
	try {
		const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
		mediaRecorder = new MediaRecorder(stream);
		
		mediaRecorder.ondataavailable = function(event) {
			audioChunks.push(event.data);
		};
		
		mediaRecorder.onstop = function() {
			audioBlob = new Blob(audioChunks, { 'type' : 'audio/wav; codecs=opus' });
			audioURL = URL.createObjectURL(audioBlob);
			audioChunks = [];
		};
		
		console.log(\"Microphone setup complete.\");
	} catch (error) {
		console.error(\"Error accessing microphone:\", error);
	}
}



window.ttsState = ttsState;
window.state = state;
window.setupMicrophone = setupMicrophone;
window.aiResponse='';
window.startListening = startListening;
window.stopListening=stopListening;
window.recognizedText = recognizedText;

</script>"
html/canvas_resize_policy=2
html/focus_canvas_on_start=true
html/experimental_virtual_keyboard=false
progressive_web_app/enabled=false
progressive_web_app/ensure_cross_origin_isolation_headers=true
progressive_web_app/offline_page=""
progressive_web_app/display=1
progressive_web_app/orientation=0
progressive_web_app/icon_144x144=""
progressive_web_app/icon_180x180=""
progressive_web_app/icon_512x512=""
progressive_web_app/background_color=Color(0, 0, 0, 1)
